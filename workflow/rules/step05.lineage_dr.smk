##### Step 05.lineage_dr #####

# Run TBProfiler whole pipeline for each sample
## including lineage typing and drug resistance detection Analysis
rule tbprofiler_profile:
    input:
        fq1 = lambda wildcards: get_fastq_reads(wildcards.sample)[0],
        fq2 = lambda wildcards: get_fastq_reads(wildcards.sample)[1]
    output:
        bam = "results/05.lineage_dr/bam/{sample}.bam",
        bai = "results/05.lineage_dr/bam/{sample}.bam.bai",
        txt = "results/05.lineage_dr/results/{sample}.results.txt",
        pdf = "results/05.lineage_dr/results/{sample}.results.pdf",
        json = "results/05.lineage_dr/results/{sample}.results.json",
        csv = "results/05.lineage_dr/results/{sample}.results.csv",
        bcf = "results/05.lineage_dr/vcf/{sample}.delly.bcf",
        vcf = "results/05.lineage_dr/vcf/{sample}.targets.csq.vcf.gz"
    params:
        out_dir = "results/05.lineage_dr",
        # --suspect             Use the suspect suite of tools to add ML predictions (default: False)
        # turn on suspect-BDQ and suspect-PZA support
        suspect = "--suspect"
    threads:
        config["threads"]
    log:
        "logs/05.lineage_dr/tbprofiler_profile/{sample}.log"
    conda:
        "../envs/TBProfiler.yaml"
    shell:
        """
        # fix python multiprocessing ERROR "OpenBLAS blas_thread_init" issue
        export OPENBLAS_NUM_THREADS=30
        tb-profiler profile -1 {input.fq1} -2 {input.fq2} -t {threads} -p {wildcards.sample} \
        -d {params.out_dir} --csv --txt --pdf {params.suspect} 2> {log}
        """

# Collate results form multiple samples together
rule tbprofiler_collate:
    input:
        sample_list = "results/00.data_cleaning/kraken.samples",
        txt = lambda wildcards: get_qualified_results("results/05.lineage_dr/results/{sample}.results.txt", wildcards),
        pdf = lambda wildcards: get_qualified_results("results/05.lineage_dr/results/{sample}.results.pdf", wildcards),
        json = lambda wildcards: get_qualified_results("results/05.lineage_dr/results/{sample}.results.json", wildcards),
        csv = lambda wildcards: get_qualified_results("results/05.lineage_dr/results/{sample}.results.csv", wildcards)
    output:
        itol_dr_indiv = "results/05.lineage_dr/all.dr.indiv.itol.txt",
        itol_dr = "results/05.lineage_dr/all.dr.itol.txt",
        itol_lineage = "results/05.lineage_dr/all.lineage.itol.txt",
        json = "results/05.lineage_dr/all.json",
        txt = "results/05.lineage_dr/all.txt",
        variants = "results/05.lineage_dr/all.variants.txt"
    params:
        prefix = "all",
        storage_dir = "results/05.lineage_dr/results"
    log:
        "logs/05.lineage_dr/tbprofiler_collate.log"
    conda:
        "../envs/TBProfiler.yaml"
    shell:
        """
        tb-profiler collate -p {params.prefix} --samples {input.sample_list} -d {params.storage_dir} --full 2> {log}
        mv all.dr.indiv.itol.txt all.dr.itol.txt all.lineage.itol.txt all.json all.txt all.variants.txt \
        results/05.lineage_dr/ 2>> {log}
        """

# convert iTOL annotation files which is generated by TB-Profiler to ggtree annotation files
rule tbprofiler_annotation_convert:
    input:
        itol_dr_indiv = "results/05.lineage_dr/all.dr.indiv.itol.txt",
        itol_dr = "results/05.lineage_dr/all.dr.itol.txt",
        itol_lineage = "results/05.lineage_dr/all.lineage.itol.txt"
    output:
        ggtree_dr_indiv = "results/05.lineage_dr/all.dr.indiv.ggtree.txt",
        ggtree_dr = "results/05.lineage_dr/all.dr.ggtree.txt",
        ggtree_lineage = "results/05.lineage_dr/all.lineage.ggtree.txt"
    params:
        script = "workflow/scripts/extract_itol_annotation.py"
    log:
        "logs/05.lineage_dr/tbprofiler_annotation_convert.log"
    conda:
        "../envs/TBProfiler.yaml"
    shell:
        "python {params.script} {input.itol_lineage} {input.itol_dr} {input.itol_dr_indiv} "
        "{output.ggtree_lineage} {output.ggtree_dr} {output.ggtree_dr_indiv} 2>&1 > {log}"
